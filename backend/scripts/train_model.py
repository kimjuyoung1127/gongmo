#!/usr/bin/env python3
"""
ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (v4)

- **ë°ì´í„° ì¤‘ì‹¬ ì ‘ê·¼ë²•**: ëª¨ë¸ ë³€ê²½ì´ ì•„ë‹Œ, ë°ì´í„° ì „ì²˜ë¦¬ ë°©ì‹ ê°œì„ ì— ì§‘ì¤‘.
- **ê³ ê¸‰ ì „ì²˜ë¦¬**: í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°(Okt)ë¥¼ ì‚¬ìš©í•´ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ í”¼ì²˜ë¡œ ì‚¬ìš©.
- **ì›ë³¸ ë°ì´í„° ì‚¬ìš©**: ë…¸ì´ì¦ˆ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì¦ê°• ë°ì´í„°ë¥¼ ì œì™¸í•˜ê³ , `food_dataset_v4_clean.csv` ì›ë³¸ ì‚¬ìš©.
- ë‹¤ì¤‘ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‹¤í—˜ ë° ìµœì  ëª¨ë¸ ì„ ì •.
"""

import pandas as pd
import os
import re
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
import lightgbm as lgb
import joblib
import json
import time

# --- Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ---
okt = Okt()

def preprocess_korean_text(text):
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜:
    1. í•œê¸€, ê³µë°± ì œì™¸ ëª¨ë“  ë¬¸ì ì œê±°
    2. Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
    3. ì¶”ì¶œëœ ëª…ì‚¬ë¥¼ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•œ ë¬¸ìì—´ ë°˜í™˜
    """
    if not isinstance(text, str):
        return ""
    # 1. í•œê¸€, ê³µë°± ì œì™¸ ëª¨ë“  ë¬¸ì ì œê±°
    text = re.sub(r'[^ê°€-í£\s]', '', text)
    # 2. ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(text)
    # 3. ê³µë°±ìœ¼ë¡œ ì—°ê²°
    return " ".join(nouns)

def train_model():
    print("ğŸ”„ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (v4 - í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ê¸°ë°˜ ê³ ê¸‰ ì „ì²˜ë¦¬)...")
    
    # --- 1. ê²½ë¡œ ë° ì„¤ì • ---
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # ì›ë³¸ ë°ì´í„°ì…‹ ì‚¬ìš©
    dataset_path = os.path.join(current_dir, '..', 'data', 'food_dataset_v4_clean.csv')
    models_dir = os.path.join(current_dir, '..', 'models')
    os.makedirs(models_dir, exist_ok=True)

    # --- 2. ë°ì´í„°ì…‹ ë¡œë“œ ---
    print(f"ğŸ“ '{os.path.basename(dataset_path)}' ë¡œë“œ ì¤‘...")
    try:
        df = pd.read_csv(dataset_path)
        df.dropna(subset=['clean_text', 'category_code'], inplace=True)
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ: {len(df):,}ê°œ ìƒ˜í”Œ, {df['category_code'].nunique()}ê°œ ì¹´í…Œê³ ë¦¬")
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {dataset_path}. ë°ì´í„°ì…‹ì„ í™•ì¸í•˜ì„¸ìš”.")
        return False

    # --- 3. ë°ì´í„° ì „ì²˜ë¦¬ (í•µì‹¬ ë³€ê²½) ---
    print("\nğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ (Okt ëª…ì‚¬ ì¶”ì¶œ)...")
    start_time = time.time()
    df['processed_text'] = df['clean_text'].apply(preprocess_korean_text)
    preprocess_time = time.time() - start_time
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {preprocess_time:.2f}ì´ˆ")
    
    # ì „ì²˜ë¦¬ í›„ ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ” ìƒ˜í”Œ ì œê±°
    df = df[df['processed_text'].str.strip().astype(bool)]
    print(f"   - ìœ íš¨ ìƒ˜í”Œ ìˆ˜: {len(df):,}ê°œ")

    # --- 4. íŠ¹ì„±(X)ê³¼ ë¼ë²¨(y) ë¶„ë¦¬ ë° ë°ì´í„° ë¶„í•  ---
    X = df['processed_text'] # ìƒˆë¡œ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ í”¼ì²˜ë¡œ ì‚¬ìš©
    y = df['category_code']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # --- 5. TF-IDF ë²¡í„°í™” ---
    print("\nğŸ“ TF-IDF ë²¡í„°í™” ì¤‘...")
    # ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì´ë¯¸ í† í°í™”ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ, TfidfVectorizerëŠ” ë‹¨ì–´ ë¹ˆë„ë§Œ ê³„ì‚°
    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.8)
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    print(f"âœ… ë²¡í„°í™” ì™„ë£Œ (íŠ¹ì„±: {X_train_vec.shape[1]}ê°œ)")

    # --- 6. ëª¨ë¸ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜ ---
    models_to_test = {
        'LogisticRegression': {
            'estimator': LogisticRegression(random_state=42, max_iter=2000, multi_class='multinomial', solver='lbfgs'),
            'params': {'C': [1, 10, 50]}
        },
        'SGDClassifier': {
            'estimator': SGDClassifier(random_state=42, loss='hinge'),
            'params': {'alpha': [0.0001, 0.001, 0.01]}
        },
        'MultinomialNB': {
            'estimator': MultinomialNB(),
            'params': {'alpha': [0.1, 0.5, 1.0]}
        },
        'LightGBM': {
            'estimator': lgb.LGBMClassifier(random_state=42),
            'params': {'n_estimators': [100, 200], 'learning_rate': [0.1, 0.2]}
        }
    }

    best_model_info = {'name': None, 'score': -1}

    # --- 7. ë‹¤ì¤‘ ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ ë£¨í”„ ---
    print("\nğŸ¤– ë‹¤ì¤‘ ëª¨ë¸ í›ˆë ¨ ë° ìµœì  ëª¨ë¸ íƒìƒ‰ ì‹œì‘...")
    for name, config in models_to_test.items():
        print(f"\n--- {name} í›ˆë ¨ ---")
        grid_search = GridSearchCV(
            estimator=config['estimator'], param_grid=config['params'], cv=5, scoring='accuracy', n_jobs=-1, verbose=1
        )
        grid_search.fit(X_train_vec, y_train)
        accuracy = grid_search.best_estimator_.score(X_test_vec, y_test)
        
        print(f"   - ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
        print(f"   - ìµœê³  êµì°¨ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.4f}")
        print(f"   - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f}")

        if accuracy > best_model_info['score']:
            print(f"   âœ¨ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë°œê²¬!")
            best_model_info.update({
                'name': name,
                'score': accuracy,
                'estimator': grid_search.best_estimator_,
                'params': grid_search.best_params_,
                'cv_score': grid_search.best_score_
            })

    if not best_model_info['name']:
        print("âŒ ëª¨ë“  ëª¨ë¸ í›ˆë ¨ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return False

    # --- 8. ìµœì¢… ìµœì  ëª¨ë¸ ì €ì¥ ---
    print(f"\nğŸ’¾ ìµœì¢… ìµœì  ëª¨ë¸ ({best_model_info['name']}) ì €ì¥ ì¤‘...")
    
    model = best_model_info['estimator']
    joblib.dump(model, os.path.join(models_dir, 'item_classifier.pkl'))
    joblib.dump(vectorizer, os.path.join(models_dir, 'vectorizer.pkl'))
    
    model_info = {
        'best_model': best_model_info['name'],
        'preprocessing_method': 'korean_noun_extraction (Okt)',
        'classes': list(model.classes_),
        'hyperparameter_tuning': {
            'best_params': best_model_info['params'],
            'best_cv_score': best_model_info['cv_score'],
        },
        'performance': {'test_accuracy': best_model_info['score']},
        'dataset_info': {
            'dataset_file': os.path.basename(dataset_path),
            'total_samples': len(df),
        }
    }
    
    with open(os.path.join(models_dir, 'model_classes.json'), 'w', encoding='utf-8') as f:
        json.dump(model_info, f, indent=2, ensure_ascii=False)
    
    print("âœ… ì €ì¥ ì™„ë£Œ.")

    # --- 9. ìµœì¢… ê²°ê³¼ ë³´ê³  ---
    print(f"\nğŸ“‹ ìµœì¢… í›ˆë ¨ ê²°ê³¼:")
    print(f"   - ğŸ† ìµœì  ëª¨ë¸: {best_model_info['name']}")
    print(f"   - âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {best_model_info['score']:.4f} ({best_model_info['score']*100:.2f}%)")
    print(f"   - âš™ï¸ ìµœì  íŒŒë¼ë¯¸í„°: {best_model_info['params']}")

    print("\nğŸ“Š ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ (ìµœì  ëª¨ë¸ ê¸°ì¤€):")
    y_pred = model.predict(X_test_vec)
    print(classification_report(y_test, y_pred, zero_division=0))
    
    if best_model_info['score'] > 0.85:
        print(f"\nğŸ‰ í›ˆë ¨ ì„±ê³µ! (ì •í™•ë„ > 85%)")
    else:
        print(f"\nâš ï¸ ì •í™•ë„ ê°œì„  í•„ìš” (ëª©í‘œ: 85%, í˜„ì¬: {best_model_info['score']*100:.1f}%)")
    
    return True

if __name__ == "__main__":
    success = train_model()
    if success:
        print("\nğŸ‰ ëª¨ë¸ í›ˆë ¨ ë° ì„ ì • ì™„ë£Œ!")
    else:
        print("\nâŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨!")
