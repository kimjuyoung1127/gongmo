#!/usr/bin/env python3
"""
ë°ì´í„°ì…‹ ì¦ê°• ìŠ¤í¬ë¦½íŠ¸ (v3)

- ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë°ì´í„° ì¦ê°• ì „ëµì„ ìˆ˜ì •í•©ë‹ˆë‹¤. (Phase 4, Action 4.1)
- categories_master.csvë¥¼ ë‹¨ì¼ ì§„ì‹¤ ì›ì²œìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- food_dataset_v4_clean.csvë¥¼ ê¸°ë°˜ìœ¼ë¡œ **êµ¬ì¡°ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë…¸ì´ì¦ˆ**ë¥¼ ì ìš©í•˜ì—¬ ë°ì´í„° ì¦ê°•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- ìµœì¢…ì ìœ¼ë¡œ food_dataset_v5_augmented.csvë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import pandas as pd
import os
import random

# --- ì„¤ì • ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(os.path.dirname(BASE_DIR), 'data')

MASTER_CATEGORIES_PATH = os.path.join(DATA_DIR, 'categories_master.csv')
CLEAN_DATASET_PATH = os.path.join(DATA_DIR, 'food_dataset_v4_clean.csv')
AUGMENTED_DATASET_PATH = os.path.join(DATA_DIR, 'food_dataset_v5_augmented.csv') # v5ë¡œ ë³€ê²½

TARGET_SAMPLE_COUNT = 15  # ì¹´í…Œê³ ë¦¬ë³„ ëª©í‘œ ìƒ˜í”Œ ìˆ˜ (10ê°œ ì´ìƒ ë³´ì¥, 15ê°œë¡œ ì„¤ì •)

def generate_structured_noise(text):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ì˜ˆì¸¡ ê°€ëŠ¥í•œ êµ¬ì¡°ì  ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€"""
    # ëª¨ë¸ì´ í•™ìŠµí•´ì•¼ í•  í•µì‹¬ì ì´ê³  í˜„ì‹¤ì ì¸ íŒ¨í„´ ìœ„ì£¼ë¡œ ì œí•œ
    patterns = [
        f"(PB){text}",
        f"í–‰ì‚¬){text}",
        f"{text} ëŒ€ìš©ëŸ‰",
        f"{text} 1+1",
        f"ì‹ ì„ ì½”ë„ˆ {text}",
        f"{text}*2",
        f"{text} 500G"
    ]
    return random.choice(patterns)

def augment_dataset():
    print("ğŸ”„ ë°ì´í„°ì…‹ ì¦ê°• ì‹œì‘ (v3 - êµ¬ì¡°ì  ë…¸ì´ì¦ˆ)...")

    # 1. ë‹¨ì¼ ì§„ì‹¤ ì›ì²œ(Master Categories) ë¡œë“œ
    try:
        print(f"ğŸ“ '{os.path.basename(MASTER_CATEGORIES_PATH)}' ë¡œë“œ ì¤‘...")
        master_categories_df = pd.read_csv(MASTER_CATEGORIES_PATH)
        standard_categories = set(master_categories_df['category_code'])
        print(f"âœ… í‘œì¤€ ì¹´í…Œê³ ë¦¬: {len(standard_categories)}ê°œ ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: '{MASTER_CATEGORIES_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ì •ì œëœ ë°ì´í„°ì…‹ ë¡œë“œ
    try:
        print(f"ğŸ“ '{os.path.basename(CLEAN_DATASET_PATH)}' ë¡œë“œ ì¤‘...")
        clean_df = pd.read_csv(CLEAN_DATASET_PATH)
        # ì›ë³¸ ë°ì´í„°ëŠ” í•­ìƒ í¬í•¨ë˜ë„ë¡ ë³´ì¥
        print(f"âœ… ì •ì œëœ ë°ì´í„°ì…‹: {len(clean_df)}ê°œ ìƒ˜í”Œ (ì´ë“¤ì€ ìµœì¢… ë°ì´í„°ì…‹ì— ëª¨ë‘ í¬í•¨ë©ë‹ˆë‹¤)")
    except FileNotFoundError:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: '{CLEAN_DATASET_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ ìˆ˜ ë¶„ì„
    category_counts = clean_df['category_code'].value_counts()
    
    # 4. ë°ì´í„° ì¦ê°•
    augmented_data = []
    print("\nğŸ”¬ ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì¦ê°• ì¤‘...")
    
    for category in sorted(list(standard_categories)):
        count = category_counts.get(category, 0)
        
        # ì›ë³¸ ë°ì´í„°ê°€ ëª©í‘œì¹˜ë³´ë‹¤ ë§ìœ¼ë©´, ì¦ê°•í•˜ì§€ ì•Šê³  ì›ë³¸ë§Œ ì‚¬ìš©
        if count >= TARGET_SAMPLE_COUNT:
            continue
            
        needed = TARGET_SAMPLE_COUNT - count
        
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ê¸°ì¡´ ìƒ˜í”Œì„ ê¸°ë°˜ìœ¼ë¡œ ì¦ê°•
        base_samples = clean_df[clean_df['category_code'] == category]['clean_text'].tolist()
        
        if not base_samples:
            print(f"   âš ï¸ ê²½ê³ : '{category}'ì— ëŒ€í•œ ê¸°ë³¸ ìƒ˜í”Œì´ ì—†ì–´ ì¦ê°•í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
            
        new_samples = []
        # ì¦ê°• ì‹œ ì¤‘ë³µì„ í”¼í•˜ê¸° ìœ„í•´ ê¸°ì¡´ ìƒ˜í”Œ + ìƒˆ ìƒ˜í”Œì„ ì¶”ì 
        existing_texts = set(base_samples)

        while len(new_samples) < needed:
            base_text = random.choice(base_samples)
            noisy_text = generate_structured_noise(base_text)
            
            # ìƒì„±ëœ ë…¸ì´ì¦ˆ í…ìŠ¤íŠ¸ê°€ ê¸°ì¡´ì— ì—†ë‹¤ë©´ ì¶”ê°€
            if noisy_text not in existing_texts:
                new_samples.append((noisy_text, category))
                existing_texts.add(noisy_text)
        
        augmented_data.extend(new_samples)
        print(f"   - {category:<25} | í˜„ì¬: {count:>3}ê°œ | ì¶”ê°€: {len(new_samples):>3}ê°œ")

    # 5. ì¦ê°•ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    augmented_df = pd.DataFrame(augmented_data, columns=['clean_text', 'category_code'])
    if not augmented_df.empty:
        print(f"\nâœ… ì´ {len(augmented_df)}ê°œì˜ ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„±")
    else:
        print("\nâœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ê°€ ëª©í‘œ ìƒ˜í”Œ ìˆ˜ë¥¼ ì¶©ì¡±í•˜ì—¬ ì¶”ê°€ ì¦ê°•ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")

    # 6. ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
    # clean_df (ì›ë³¸)ì€ í•­ìƒ í¬í•¨ë˜ë¯€ë¡œ, ë°ì´í„° ìœ ì‹¤ ì—†ìŒ
    print("ğŸ”— ì›ë³¸ ë°ì´í„°ì™€ ì¦ê°• ë°ì´í„° ë³‘í•© ì¤‘...")
    final_df = pd.concat([clean_df, augmented_df], ignore_index=True)
    
    # ìµœì¢… ë°ì´í„°ì…‹ì—ì„œ ì¤‘ë³µ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„)
    final_df.drop_duplicates(subset=['clean_text'], inplace=True, keep='first')

    # 7. ìµœì¢… ê²°ê³¼ ì €ì¥
    try:
        print(f"\nğŸ’¾ '{os.path.basename(AUGMENTED_DATASET_PATH)}' ì €ì¥ ì¤‘...")
        final_df.to_csv(AUGMENTED_DATASET_PATH, index=False)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {AUGMENTED_DATASET_PATH}")
        print(f"   - ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(final_df)}ê°œ")
        print(f"   - ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆ˜: {final_df['category_code'].nunique()}ê°œ")
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return

    print("\nğŸ‰ ë°ì´í„° ì¦ê°• ì™„ë£Œ!")

if __name__ == "__main__":
    augment_dataset()