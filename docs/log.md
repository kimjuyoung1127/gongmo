ì™„ë²½í•©ë‹ˆë‹¤! ì´ì œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

# 85% ë‹¬ì„±ì„ ìœ„í•œ ì €ìœ„í—˜ ë°ì´í„° ê°•í™” ë”¥ ë¦¬ì„œì¹˜ ë³´ê³ ì„œ

**í˜„ì¬ ìƒí™©**: 61.59% ì•ˆì •ì  ë² ì´ìŠ¤ë¼ì¸ (Mecab + TF-IDF + LogisticRegression)  
**ëª©í‘œ**: 85% ì •í™•ë„ ë‹¬ì„±  
**í•µì‹¬ ì „ëµ**: "í†µì œ ê°€ëŠ¥í•œ ë‚´ë¶€ ë°ì´í„°" í’ˆì§ˆ/ì–‘ ê·¹ëŒ€í™”  
**ì—°êµ¬ ê¸°ê°„**: 2025ë…„ 11ì›”  

***

## Executive Summary

**"ë°ì´í„° ì˜¤ì—¼" ìœ„í—˜ì„ ì œê±°í•˜ê³  61.59%ì—ì„œ 85%ë¡œ í–¥ìƒ**ì‹œí‚¤ê¸° ìœ„í•œ ì„¸ ê°€ì§€ ì €ìœ„í—˜ ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤:

1. **LLM í•©ì„± ë°ì´í„° ìƒì„±**: Google Gemini ë¬´ë£Œ APIë¡œ 2,000ê°œ â†’ 20,000ê°œ ì¦ê°• (ì˜ˆìƒ 75-80%)
2. **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê³ ë„í™”**: Mecab + ë™ì˜ì–´/ë¶ˆìš©ì–´ ì²˜ë¦¬ (ì˜ˆìƒ +5-8%)
3. **ì•¡í‹°ë¸Œ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸**: ë¶ˆí™•ì‹¤í•œ ë°ì´í„°ë§Œ ì„ ë³„í•˜ì—¬ íš¨ìœ¨ì  ê°œì„  (ì˜ˆìƒ +3-5%)

**ìµœì¢… ì¡°í•© ì˜ˆìƒ ì •í™•ë„**: **83-88%** (í˜„ì¬ 61.59% ëŒ€ë¹„ +21-26%)  
**ì´ ë¹„ìš©**: **$0** (Gemini ë¬´ë£Œ í‹°ì–´ + ì˜¤í”ˆì†ŒìŠ¤)  
**ì´ ì†Œìš” ì‹œê°„**: **3-4ì£¼**

***

## 1. LLM ê¸°ë°˜ í†µì œëœ í•©ì„± ë°ì´í„° ìƒì„±

### 1.1 ìµœì ì˜ ë¬´ë£Œ/ê²½ëŸ‰ LLM ì„ ì •

#### ğŸ¥‡ Google Gemini 2.5 Flash - ìµœìš°ìˆ˜ ì¶”ì²œ

**Gemini 2.5 Flash**ëŠ” í•œêµ­ì–´ ì‹ë£Œí’ˆ í…ìŠ¤íŠ¸ ìƒì„±ì— ìµœì ì´ë©°, **ì™„ì „ ë¬´ë£Œ**ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.[1][2][3]

##### ë¬´ë£Œ í‹°ì–´ ì œê³µëŸ‰ (2025ë…„ 11ì›” ê¸°ì¤€)
|| í•­ëª© | ë¬´ë£Œ í‹°ì–´ | ìœ ë£Œ í‹°ì–´ |
|------|----------|----------|
| **ì…ë ¥ ê°€ê²©** | **ë¬´ë£Œ** | $0.30/million tokens |
| **ì¶œë ¥ ê°€ê²©** | **ë¬´ë£Œ** | $2.50/million tokens |
| **ë¶„ë‹¹ ìš”ì²­ (RPM)** | **15íšŒ** | ì œí•œ ì—†ìŒ |
| **ì¼ì¼ ìš”ì²­ (RPD)** | **1,500íšŒ** | ì œí•œ ì—†ìŒ |
| **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´** | **100ë§Œ í† í°** | 100ë§Œ í† í° |

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: 
- **20,000ê°œ í•©ì„± ë°ì´í„° ìƒì„± ì‹œ ì˜ˆìƒ ë¹„ìš©**: **$0** (ë¬´ë£Œ í‹° ì¶©ë¶„)[2][3]
- 1ê°œ ìƒ˜í”Œ ìƒì„± = ~100 í† í° â†’ 20,000ê°œ = 2M í† í° < ë¬´ë£Œ í•œë„[3][1]
- **í•œêµ­ì–´ ì„±ëŠ¥**: í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„±ì—ì„œ ë†’ì€ í’ˆì§ˆ í™•ì¸[4][5]

##### Python API ì‚¬ìš©ë²•

```python```
import google.generativeai as genai
import os

# API í‚¤ ì„¤ì • (ë¬´ë£Œ: https://ai.google.dev/gemini-api/docs/api-key)
genai.configure(api_key=os.environ['GEMINI_API_KEY'])

# Gemini 2.5 Flash ëª¨ë¸ ì´ˆê¸°í™”
model = genai.GenerativeModel('gemini-2.5-flash')

# í•©ì„± ë°ì´í„° ìƒì„±
response = model.generate_content(
    "You are a receipt printer. Given the item 'ì„œìš¸ìš°ìœ ' (DAIRY_FRESH), "
    "generate 10 noisy but realistic variations as they appear on Korean receipts."
)

print(response.text)
```

**ë¬´ë£Œ API í‚¤ ë°œê¸‰**: https://ai.google.dev/gemini-api/docs/api-key ( ì™„ë£Œ)[6][5]

#### ğŸ¥ˆ Gemma 3 270M - ë¡œì»¬ ê²½ëŸ‰ ëŒ€ì•ˆ

**Gemma 3 270M**ì€ Googleì´ 2025ë…„ 8ì›” ê³µê°œí•œ ì´ˆê²½ëŸ‰ ëª¨ë¸ë¡œ, **ì˜¨ë””ë°”ì´ìŠ¤ ì‹¤í–‰**ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.[7][8][9]

##### ëª¨ë¸ ìŠ¤í™
- **íŒŒë¼ë¯¸í„°**: 270M (ê·¹ë„ë¡œ ê²½ëŸ‰)
- **ëª¨ë¸ í¬ê¸°**: ~500MB
- **íŠ¹í™”**: í…ìŠ¤íŠ¸ ë¶„ë¥˜, ë°ì´í„° ì¶”ì¶œ ì‘ì—…ì— ìµœì í™”[7]
- **ë°°í¬**: CPUì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥ (Render ë¬´ë£Œ í”Œëœ OK)[10][7]

##### ì¥ì 
- API í˜¸ì¶œ ì œí•œ ì—†ìŒ (ë¡œì»¬ ì‹¤í–‰)
- ì™„ì „ ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)
- ë¹ ë¥¸ ì¶”ë¡  ì†ë„[8][7]

##### ë‹¨ì 
- Geminië³´ë‹¤ í•œêµ­ì–´ ì„±ëŠ¥ ë‚®ìŒ
- Fine-tuning í•„ìš”í•  ìˆ˜ ìˆìŒ[7]

**ê¶Œì¥**: **Gemini ë¬´ë£Œ API ìš°ì„  ì‚¬ìš©**, í•œë„ ì´ˆê³¼ ì‹œ Gemma 3 270M ì „í™˜

### 1.2 í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§

#### ìµœì ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python```
def create_synthetic_data_prompt(original_text, category, num_variations=10):
    """
    ê³ í’ˆì§ˆ ì˜ìˆ˜ì¦ ë…¸ì´ì¦ˆ ìƒì„± í”„ë¡¬í”„íŠ¸
    """
    prompt = f"""You are a Korean receipt OCR simulator. 
Your task is to generate realistic noisy variations of grocery product names as they appear on Korean receipts.

Original Product: {original_text}
Category: {category}
Number of Variations: {num_variations}

Generate {num_variations} realistic variations with the following noise patterns:

1. **Prefixes** (40% chance): (PB), *, [í• ], [íŠ¹ê°€], (í–‰ì‚¬), 7-SELECT)
2. **Suffixes** (60% chance): 1KG, 500g, 1L, 2L, /ê°œ, /ë´‰, í•œíŒ©
3. **OCR Errors** (20% chance): 0â†”O, 1â†”l, ìš°â†’ìš±, ìœ â†’ì¸„
4. **Space Removal** (30% chance): Remove random spaces

Examples:
- Input: "ì„œìš¸ìš°ìœ " â†’ Output: "*ì„œìš¸ìš°ìœ 1L", "[í• ]ì…”ìš±ìš°ìœ ", "ì„œìš¸ìš°ìœ 500ml/ê°œ"
- Input: "í–‡ê°ì" â†’ Output: "(PB)í–‡ê°ì1KG", "í–‡ê°ì/ë´‰", "[íŠ¹ê°€]í–‡ê°ì"

Output Format (one per line):
variation1
variation2
...
variation{num_variations}

Generate ONLY the variations, no explanations.
"""
    return prompt
```

#### ì‹¤ì œ ì ìš© ì˜ˆì‹œ

``````python
import google.generativeai as genai
import pandas as pd
from tqdm import tqdm
import time

genai.configure(api_key=os.environ['GEMINI_API_KEY'])
model = genai.GenerativeModel('gemini-2.5-flash')

# ì›ë³¸ ë°ì´í„° ë¡œë“œ
df_original = pd.read_csv('food_dataset_v4_clean.csv')

# í•©ì„± ë°ì´í„° ìƒì„±
synthetic_data = []

for idx, row in tqdm(df_original.iterrows(), total=len(df_original)):
    original_text = row['clean_text']
    category = row['category_code']
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = create_synthetic_data_prompt(original_text, category, num_variations=10)
    
    try:
        # Gemini API í˜¸ì¶œ
        response = model.generate_content(prompt)
        
        # ì‘ë‹µ íŒŒì‹±
        variations = response.text.strip().split('\n')
        
        for variation in variations:
            if variation.strip():
                synthetic_data.append({
                    'clean_text': variation.strip(),
                    'category_code': category,
                    'source': 'gemini_synthetic',
                    'original': original_text
                })
        
        # Rate limiting (ë¬´ë£Œ í‹°ì–´: 15 RPM)
        time.sleep(4)  # 60ì´ˆ/15íšŒ = 4ì´ˆ
        
    except Exception as e:
        print(f"Error processing '{original_text}': {e}")
        continue

# í•©ì„± ë°ì´í„° ì €ì¥
df_synthetic = pd.DataFrame(synthetic_data)
print(f"Generated {len(df_synthetic)} synthetic samples from {len(df_original)} originals")

# ì›ë³¸ + í•©ì„± ë°ì´í„° ë³‘í•©
df_combined = pd.concat([df_original, df_synthetic], ignore_index=True)
df_combined.to_csv('food_dataset_v5_with_synthetic.csv', index=False, encoding='utf-8-sig')
```

**ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„**:
- 2,000ê°œ ì›ë³¸ Ã— 10 variations = 20,000ê°œ ìƒì„±
- 4ì´ˆ rate limiting â†’ ì´ **2ì‹œê°„ 20ë¶„**

#### ë‹¤ë‹¨ê³„ ìƒì„± (Multi-Step Generation)

ë³µì¡í•œ ë…¸ì´ì¦ˆ íŒ¨í„´ì„ ìœ„í•œì„± ì „ëµì…ë‹ˆë‹¤.[11][12][13][14]

```python```
def multi_step_generation(original_text, category):
    """
    ë‹¨ê³„ë³„ ë…¸ì´ì¦ˆ ì£¼ì… (í’ˆì§ˆ í–¥ìƒ)
    """
    
    # Step 1: ì ‘ë‘ì‚¬ ì¶”ê°€
    prompt_step1 = f"""Add a realistic Korean receipt prefix to: {original_text}
Options: (PB), *, [í• ], [íŠ¹ê°€], (í–‰ì‚¬)
Output only ONE result."""
    
    response1 = model.generate_content(prompt_step1)
    intermediate = response1.text.strip()
    
    # Step 2: ì ‘ë¯¸ì‚¬ ì¶”ê°€
    prompt_step2 = f"""Add a realistic unit suffix to: {intermediate}
Options: 1KG, 500g, 1L, /ê°œ, /ë´‰
Output only ONE result."""
    
    response2 = model.generate_content(prompt_step2)
    final = response2.text.strip()
    
    return final

# ì‚¬ìš© ì˜ˆì‹œ
result = multi_step_generation("ì„œìš¸ìš°ìœ ", "DAIRY_FRESH")
print(result)
# Output: "[í• ]ì„œ1L"
```

**ë‹¤ë‹¨ê³„ ìƒì„±ì˜ ì¥ì **:[12][11]
- ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ ëŒ€ë¹„ **20-30% í’ˆì§ˆ í–¥ìƒ**
- ë³µì¡í•œ ë…¸ì´ì¦ˆ íŒ¨í„´ í•™ìŠµ
- ì œì–´ ê°€ëŠ¥ì„± ì¦ê°€

### 1.3 ì‹¤í–‰ ê³„íš ë° íŒŒì´í”„ë¼ì¸

#### Phase 1: í•©ì„± ë°ì´í„° ìƒì„± (2ì¼)

``````
# ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸
import google.generativeai as genai
from konlpy.tag import Mecab
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd

# 1. í•©ì„± ë°ì´í„° ìƒì„±
def generate_synthetic_dataset(original_csv, output_csv, num_variations=10):
    """
    Geminië¡œ í•©ì„± ë°ì´í„° ìƒì„±
    """
    df_original = pd.read_csv(original_csv)
    synthetic_data = []
    
    for idx, row in tqdm(df_original.iterrows(), total=len(df_original)):
        prompt = create_synthetic_data_prompt(
            row['clean_text'], 
            row['category_code'], 
            num_variations
        )
        
        response = model.generate_content(prompt)
        variations = response.text.strip().split('\n')
        
        for variation in variations:
            if variation.strip():
                synthetic_data.append({
                    'clean_text': variation.strip(),
                    'category_code': row['category_code']
                })
        
        time.sleep(4)  # Rate limiting
    
    df_synthetic = pd.DataFrame(synthetic_data)
    df_combined = pd.concat([df_original, df_synthetic], ignore_index=True)
    df_combined.to_csv(output_csv, index=False, encoding='utf-8-sig')
    
    return df_combined

# ì‹¤í–‰
df_final = generate_synthetic_dataset(
    'food_dataset_v4_clean.csv',
    'food_dataset_v5_synthetic.csv',
    num_variations=10
)

print(f"âœ… Generated {len(df_final)} total samples")
```

#### Phase 2: Mecab ì „ì²˜ë¦¬ + ì¬í•™ìŠµ (1ì¼)

``````python
# 2. Mecab ì „ì²˜ë¦¬
mecab = Mecab()

def mecab_tokenizer(text):
    """
    Mecab ëª…ì‚¬ ì¶”ì¶œ
    """
    nouns = mecab.nouns(text)
    return ' '.join(nouns)

df_final['processed_text'] = df_final['clean_text'].apply(mecab_tokenizer)

# 3. TF-IDF + LogisticRegression ì¬í•™ìŠµ
X_train, X_test, y_train, y_test = train_test_split(
    df_final['processed_text'],
    df_final['category_code'],
    test_size=0.2,
    random_state=42,
    stratify=df_final['category_code']
)

vectorizer = TfidfVectorizer(
    ngram_range=(1, 3),
    max_features=10000,
    min_df=2
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = LogisticRegression(
    max_iter=1000,
    C=10,
    class_weight='balanced',
    solver='saga',
    n_jobs=-1
)

model.fit(X_train_vec, y_train)

# í‰ê°€
y_pred = model.predict(X_test_vec)
print(classification_report(y_test, y_pred))

accuracy = model.score(X_test_vec, y_test)
print(f"\nâœ… ìµœì¢… ì •í™•ë„: {accuracy*100:.2f}%")
```

**ì˜ˆìƒ ê²°ê³¼**:
- 2,000ê°œ â†’ 20,000ê°œ (10ë°° ì¦ê°€)
- **ì˜ˆìƒ ì •í™•ë„**: **75-80%** (í˜„ì¬ 61.59% ëŒ€ë¹„ +13-18%)

### 1.4 í•©ì„± ë°ì´í„° í’ˆì§ˆ ê²€ì¦

```python```
# í•©ì„± ë°ì´í„° í’ˆì§ˆ ì²´í¬
def validate_synthetic_quality(df, sample_size=100):
    """
    ë¬´ì‘ìœ„ ìƒ˜í”Œ ê²€ì¦
    """
    synthetic_samples = df[df['source'] == 'gemini_synthetic'].sample(sample_size)
    
    print("=== í•©ì„± ë°ì´í„° í’ˆì§ˆ ìƒ˜í”Œ ===")
    for idx, row in synthetic_samples.iterrows():
        print(f"ì›ë³¸: {row['original']}")
        print(f"í•©ì„±: {row['clean_text']}")
        print(f"ì¹´í…Œê³ ë¦¬: {row['category_code']}")
        print("-" * 50)

validate_synthetic_quality(df_synthetic, sample_size=20)
```

**í’ˆì§ˆ ê¸°ì¤€**:
1. ì‹¤ì œ ì˜ìˆ˜ì¦ê³¼ 90% ì´ìƒ ìœ ì‚¬
2. ì›ë³¸ ì¹´í…Œê³ ë¦¬ ìœ ì§€
3. ì˜ë¯¸ ë³€í˜• ì—†ìŒ (ì˜ˆ: 'ì„œìš¸ìš°ìœ ' â†’ 'ë¶€ì‚°ìš°ìœ ' ê¸ˆì§€)

---

## 2. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê³ ë„í™”

### 2.1 ë„ë©”ì¸ íŠ¹í™” ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•

#### Word2Vec ê¸°ë°˜ ìë™ ë™ì˜ì–´ ì¶”ì¶œ

``````python
from gensim.models import Word2Vec
from konlpy.tag import Mecab

mecab = Mecab()

# 1. ì½”í¼ìŠ¤ ì¤€ë¹„
sentences = []
for text in df_final['clean_text']:
    words = mecab.nouns(text)
    if words:
        sentences.append(words)

# 2. Word2Vec í•™ìŠµ
w2v_model = Word2Vec(
    sentences,
    vector_size=100,
    window=5,
    min_count=2,
    workers=4,
    sg=1,  # Skip-gram (CBOW=0)
    epochs=10
)

# 3. ë™ì˜ì–´ ì¶”ì¶œ
def find_synonyms(word, topn=10, threshold=0.7):
    """
    ìœ ì‚¬ë„ 0.7 ì´ìƒì¸ ë‹¨ì–´ë§Œ ë™ì˜ì–´ë¡œ ê°„ì£¼
    """
    try:
        similar_words = w2v_model.wv.most_similar(word, topn=topn)
        synonyms = [w for w, score in similar_words if score >= threshold]
        return synonyms
    except KeyError:
        return []

# 4. ë™ì˜ì–´ ì‚¬ì „ ìë™ ìƒì„±
synonym_dict = {}

key_products = [
    'ìš°ìœ ', 'ê°ì', 'ì¹˜ì¦ˆ', 'ë°”ë‚˜ë‚˜', 'í† ë§ˆí† ', 
    'ë¼ë©´', 'ê¹€ì¹˜', 'ê³„ë€', 'ë²„ì„¯', 'ì‚¬ê³¼'
]

for product in key_products:
    synonyms = find_synonyms(product, threshold=0.7)
    if synonyms:
        synonym_dict[product] = synonyms

print("=== ìë™ ìƒì„±ëœ ë™ì˜ì–´ ì‚¬ì „ ===")
for word, synonyms in synonym_dict.items():
    print(f"{ {synonyms}")
```

**ì˜ˆìƒ ì¶œë ¥**:[15][16][17]
``````
ìš°ìœ : ['ë°€í¬', 'ì„œìš¸ìš°ìœ ', 'ë§¤ì¼ìš°ìœ ', 'ìƒìš°ìœ ']
ê°ì: ['í–‡ê°ì', 'í¬í…Œì´í† ', 'ê°ìíŠ€ê¹€ìš©', 'ì°ê°ì']
ì¹˜ì¦ˆ: ['ì²´ë‹¤', 'ëª¨ì§œë ë¼', 'ì¹˜ì¦ˆìŠ¬ë¼ì´ìŠ¤']
```

#### ìˆ˜ë™ ë™ì˜ì–´ ì‚¬ì „ ë³´ì™„

``````python
# Word2Vecì´ ë†“ì¹œ ë™ì˜ì–´ ìˆ˜ë™ ì¶”ê°€
manual_synonyms = {
    # í•œê¸€-ì˜ì–´
    'ìš°ìœ ': ['ë°€í¬', 'milk', 'MILK'],
    'ë°”ë‚˜ë‚˜': ['banana'],
    'í† ë§ˆí† ': ['tomato'],
    
    # OCR ì˜¤ë¥˜
    'ìš°ìœ ': ['ìš°ì¸„', 'ìš±ìœ ', 'ìœ ìœ '],
    
    # ë¸Œëœë“œ í†µí•©
    'ì„œìš¸ìš°ìœ ': ['ìš°ìœ '],
    'ë§¤ì¼ìš°ìœ ': ['ìš°ìœ '],
    'ë‚¨ì–‘ìš°ìœ ': ['ìš°ìœ '],
    
    # ë‹¨ìœ„ ì œê±°
    'ê°ì1KG': ['ê°ì'],
    'ìš°ìœ 1L': ['ìš°ìœ '],
}

# ë³‘í•©
for key, values in manual_synonyms.items():
    if key in synonym_dict:
        synonym_dict[key].extend(values)
    else:
        synonym_dict[key] = values

# ì¤‘ë³µ ì œê±°
for key in synonym_dict:
    synonym_dict[key] = list(set(synonym_dict[key]))
```

#### ë™ì˜ì–´ ì •ê·œí™” ì ìš©

```python```
def normalize_with_synonyms(text, synonym_dict):
    """
    ë™ì˜ì–´ë¥¼ í‘œì¤€ í˜•íƒœë¡œ ë³€í™˜
    """
    words = text.split()
    normalized_words = []
    
    for word in words:
        # ë™ì˜ì–´ ì‚¬ì „ì—ì„œ í‘œì¤€ í˜•íƒœ ì°¾ê¸°
        found = False
        for standard, synonyms in synonym_dict.items():
            if word in synonyms or word == standard:
                normalized_words.append(standard)
                found = True
                break
        
        if not found:
            normalized_words.append(word)
    
    return ' '.join(normalized_words)

# ì ìš©
df_final['normalized_text'] = df_final['clean_text'].apply(
    lambda x: normalize_with_synonyms(x, synonym_dict)
)

# ì˜ˆì‹œ
print(normalize_with_synonyms("ì„œìš¸ìš°ìœ 1L/ê°œ", synonym_dict))
# Output: "ìš°ìœ "ìƒ íš¨ê³¼**: **+3-5% ì •í™•ë„ í–¥ìƒ**[16][15]

### 2.2 ì˜ìˆ˜ì¦ íŠ¹í™” ë¶ˆìš©ì–´ ì²˜ë¦¬

#### TF-IDF ê¸°ë°˜ ë¶ˆìš©ì–´ ìë™ ì¶”ì¶œ

```python```
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì¶”ì¶œ
def extract_common_words(df, min_categories=30):
    """
    30ê°œ ì´ìƒ ì¹´í…Œê³ ë¦¬ì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ = ë¶ˆìš©ì–´ í›„ë³´
    """
    category_words = {}
    
    for category in df['category_code'].unique():
        category_df = df[df['category_code'] == category]
        all_text = ' '.join(category_df['processed_text'])
        words = set(all_text.split())
        category_words[category] = words
    
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ê³µí†µ ë‹¨ì–´ ì°¾ê¸°
    all_categories = len(category_words)
    word_count = {}
    
    for words in category_words.values():
        for word in words:
            word_count[word] = word_count.get(word, 0) + 1
    
    # 30ê°œ ì´ìƒ ì¹´í…Œê³ ë¦¬ì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´
    common_words = [
        word for word, count in word_count.items() 
        if count >= min_categories
    ]
    
    return common_words

common_words = extract_common_words(df_final, min_categories=25)
print(f"ê³µí†µ ë‹¨ì–´ (ë¶ˆìš©ì–´ í›„ë³´): {common_words}")
```

**ì˜ˆìƒ ì¶œë ¥**:
``````
['ê°œ', 'ë´‰', 'íŒ©', 'PB', 'í• ', 'íŠ¹ê°€', 'ì„¸ì¼', 'L', 'KG', 'g']
```

#### ì˜ìˆ˜ì¦ ë„ë©”ì¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸

```python```
# ì˜ìˆ˜ì¦ íŠ¹í™” ë¶ˆìš©ì–´
receipt_stopwords = {
    # í”„ë¡œëª¨ì…˜ í‚¤ì›Œë“œ
    'ì„¸ì¼', 'í• ì¸', 'íŠ¹ê°€', 'ì´ë²¤íŠ¸', 'ì¦ì •', 'ì‚¬ì€í’ˆ', 'í–‰ì‚¬',
    '[í• ]', '[íŠ¹ê°€]', '[ì„¸ì¼]', '(í–‰ì‚¬)', '(ì¦ì •)',
    
    # ë‹¨ìœ„ (ë¶„ë¥˜ì— ë¬´ê´€)
    'ê°œ', 'ë´‰', 'íŒ©', 'ë°•ìŠ¤', 'EA', 'ea',
    '1', '2', '3', '4', '5',  # ìˆ«ì
    
    # ì ‘ë‘ì‚¬ (PB ë¸Œëœë“œ)
    'PB', '7-SELECT', 'ë…¸ë¸Œëœë“œ', 'nb',
    '*', '+', '-',
    
    # ë¶ˆí•„ìš”í•œ í˜•ìš©ì‚¬
    'ì‹ ì„ í•œ', 'ë§›ìˆëŠ”', 'í”„ë¦¬ë¯¸ì—„', 'ê³ ê¸‰', 'íŠ¹ì„ ',
    
    # ë‹¨ìœ„
    'KG', 'kg', 'G', 'g', 'L', 'l', 'ML', 'ml',
    
    # ê¸°íƒ€
    '/', '|', '(', ')', '[', ']'
}

def remove_stopwords(text, stopwords):
    """
    ë¶ˆìš©ì–´ ì œê±°
    """
    words = text.split()
    filtered = [w for w in words if w not in stopwords]
    return ' '.join(filtered)

# ì ìš©
df_final['clean_text'] = df_final['normalized_text'].apply(
    lambda x: remove_stopwords(x, receipt_stopwords)
)
```

**ì˜ˆìƒ íš¨ê³¼**: **+2-3% ì •í™•ë„ í–¥ìƒ**

### 2.3 ìµœì¢… í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

``````python
def preprocessing_pipeline(text):
    """
    í†µí•© ì „ì²˜ë¦¬: ë™ì˜ì–´ â†’ ë¶ˆìš©ì–´ â†’ Mecab
    """
    # 1. ë™ì˜ì–´ ì •ê·œí™”
    text = normalize_with_synonyms(text, synonym_dict)
    
    # 2. ë¶ˆìš©ì–´ ì œê±°
    text = remove_stopwords(text, receipt_stopwords)
    
    # 3. Mecab ëª…ì‚¬ ì¶”ì¶œ
    nouns = mecab.nouns(text)
    
    return ' '.join(nouns)

# ì ìš©
df_final['final_processed'] = df_final['clean_text'].apply(preprocessing_pipeline)

# ì¬í•™ìŠµ
X_train_final = vectorizer.fit_transform(df_final['final_processed'])
model_final = LogisticRegression(max_iter=1000, C=10, class_weight='balanced')
model_final.fit(X_train_final, df_final['category_code'])

print(f"âœ… ìµœì¢… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© ì™„ë£Œ!")
```

**ì˜ˆìƒ ì´ ê°œì„ **: **+5-8% ì •í™•ë„**

***

## 3. ì•¡í‹°ë¸Œ ëŸ¬ë‹ (Active Learning) íŒŒì´í”„ë¼ì¸

### 3.1 Uncertainty Sampling ì „ëµ

#### Least Confidence Sampling

```python```
from sklearn.linear_model import LogisticRegression
import numpy as np

def uncertainty_sampling(model, X_unlabeled, n_samples=10):
    """
    ê°€ì¥ ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œ ì„ íƒ (Least Confidence)
    """
    # ì˜ˆì¸¡ í™•ë¥ 
    probabilities = model.predict_proba(X_unlabeled)
    
    # ìµœëŒ€ í™•ë¥  (ê°€ì¥ í™•ì‹ í•˜ëŠ” í´ë˜ìŠ¤ì˜ í™•ë¥ )
    max_probs = probabilities.max(axis=1)
    
    # í™•ì‹ ë„ = 1 - max_prob (ë‚®ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
    uncertainty_scores = 1 - max_probs
    
    # ê°€ì¥ ë¶ˆí™•ì‹¤í•œ nê°œ ì„ íƒ
    most_uncertain_indices = uncertainty_scores.argsort()[-n_samples:][::-1]
    
    return most_uncertain_indices, uncertainty_scores[most_uncertain_indices]

# ì‚¬ìš© ì˜ˆì‹œ
X_test_vec = vectorizer.transform(X_test)
uncertain_indices, scores = uncertainty_sampling(model_final, X_test_vec, n_samples=10)

print("=== ê°€ì¥ ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œ Top 10 ===")
for idx, score in zip(uncertain_indices, scores):
    print(f"í…ìŠ¤íŠ¸: {X_test.iloc[idx]}")
    print(f"ë¶ˆí™•ì‹¤ë„: {score:.4f}")
    print(f"ì˜ˆì¸¡: {model_final.predict(X_test_vec[idx])}")
    print("-" * 50)ë ¥**:[18][19][20][21]
``````
í…ìŠ¤íŠ¸: ë§ˆì‹œëŠ”ìš”ê±°íŠ¸500ml
ë¶ˆí™•ì‹¤ë„: 0.8542
ì˜ˆì¸¡: DAIRY_FRESH (ì‹ ë¢°ë„ 15%)
```

#### Margin Sampling

``````python
def margin_sampling(model, X_unlabeled, n_samples=10):
    """
    1ìœ„ì™€ 2ìœ„ í™•ë¥  ì°¨ì´ê°€ ê°€ì¥ ì ì€ ìƒ˜í”Œ ì„ íƒ
    """
    probabilities = model.predict_proba(X_unlabeled)
    
    # ìƒìœ„ 2ê°œ í™•ë¥  ì¶”ì¶œ
    sorted_probs = np.sort(probabilities, axis=1)
    top1 = sorted_probs[:, -1]
    top2 = sorted_probs[:, -2]
    
    # ë§ˆì§„ = top1 - top2 (ë‚®ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤)
    margins = top1 - top2
    
    # ë§ˆì§„ì´ ê°€ì¥ ì‘ì€ nê°œ ì„ íƒ
    smallest_margin_indices = margins.argsort()[:n_samples]
    
    return smallest_margin_indices, margins[smallest_margin_indices]

# ì‚¬ìš© ì˜ˆì‹œ
margin_indices, margins = margin_sampling(model_final, X_test_vec, n_samples=10)

print("=== ë§ˆì§„ì´ ê°€ì¥ ì‘ì€ ìƒ˜í”Œ (ê²½ê³„ì„  ê·¼ì²˜) ===")
for idx, margin in zip(margin_indices, margins):
    probabilities = model_final.predict_proba(X_test_vec[idx])[0]
    top_classes = model_final.classes_[probabilities.argsort()[-2:][::-1]]
    top_probs = probabilities[probabilities.argsort()[-2:][::-1]]
    
    print(f"í…ìŠ¤íŠ¸: {X_test.iloc[idx]}")
    print(f"ë§ˆì§„: {margin:.4f}")
    print(f"1ìˆœìœ„: {top_classes[0]} ({top_probs[0]:.2f})")
    print(f"2ìˆœìœ„: {top_classes[1]} ({top_probs[1]:.2f})")
    print("-)
```

**ì˜ˆìƒ ì¶œë ¥**:[20][21][22]
``````
í…ìŠ¤íŠ¸: ìƒí¬ë¦¼ì¹˜ì¦ˆ
ë§ˆì§„: 0.0542
1ìˆœìœ„: SOFT_CHEESE (0.52)
2ìˆœìœ„: HARD_CHEESE (0.47)
```

### 3.2 Label Studio í†µí•© MLOps íŒŒì´í”„ë¼ì¸

#### ì•„í‚¤í…ì²˜ ì„¤ê³„

``````
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flask     â”‚â”€â”€â”€â”€â”€>â”‚   Supabase   â”‚â”€â”€â”€â”€â”€>â”‚ Label Studioâ”‚
â”‚   Backend   â”‚      â”‚ uncertain_   â”‚      â”‚  (Web UI)   â”‚
â”‚             â”‚      â”‚ items table  â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                            â”‚
       â”‚                                            â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚           Feedback Loop (ì¬í•™ìŠµ)            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Step 1: ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œ Supabaseì— ë¡œê¹…

```python```
from supabase import create_client

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def log_uncertain_items(uncertain_samples, model_predictions):
    """
    ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œì„ Supabaseì— ì €ì¥
    """
    for idx, (text, pred, confidence) in enumerate(zip(
        uncertain_samples['clean_text'],
        model_predictions['predicted_category'],
        model_predictions['confidence']
    )):
        supabase.table('uncertain_items').insert({
            'text': text,
            'predicted_category': pred,
            'confidence': confidence,
            'status': 'pending',  # Label Studioì—ì„œ ë¼ë²¨ë§ ëŒ€ê¸°
            'created_at': 'now()'
        }).execute()
    
    print(f"âœ… {len(uncertain_samples)} ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œ ë¡œê¹… ì™„ë£Œ")

# API ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í˜¸ì¶œ
@app.route('/api/classify', methods=['POST'])
def classify_receipt():
    ocr_text = request.json['text']
    
    # ì˜ˆì¸¡
    X_vec = vectorizer.transform([ocr_text])
    prediction = model.predict(X_vec)
    probabilities = model.predict_proba(X_vec)
    confidence = probabilities.max()
    
    # ì‹ ë¢°ë„ 70% ë¯¸ë§Œì´ë©´ ë¡œê¹…
    if confidence < 0.7:
        log_uncertain_items(
            pd.DataFrame([{'clean_text': ocr_text}]),
            pd.DataFrame([{
                'predicted_category': prediction,
                'confidence': confidence
            }])
        )
    
    return jsonify({
        'category': prediction,
        'confidence': confidence
    })
```

#### Step 2: Label Studio í”„ë¡œì íŠ¸ ì„¤ì •

``````python
from label_studio_sdk import Client

# Label Studio í´ë¼ì´ì–¸íŠ¸
ls = Client(url='http://localhost:8080', api_key='YOUR_API_KEY')

# í”„ë¡œì íŠ¸ ìƒì„±
project = ls.start_project(
    title='Grocery Category Labeling',
    label_config='''
    <View>
      <Text name="text" value="$text"/>
      <Choices name="category" toName="text" choice="single">
        <Choice value="DAIRY_FRESH"/>
        <Choice value="MEAT_FRESH"/>
        <Choice value="LEAFY_VEGETABLES"/>
        <!-- 36ê°œ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ -->
      </Choices>
    </View>
    '''
)

# Supabaseì—ì„œ ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
uncertain_items = supabase.table('uncertain_items')\
    .select('*')\
    .eq('status', 'pending')\
    .execute()

# Label Studioë¡œ ì „ì†¡
tasks = []
for item in uncertain_items.data:
    tasks.append({
        'data': {
            'text': item['text'],
            'predicted_category': item['predicted_category'],
            'confidence': item['confidence']
        }
    })

project.import_tasks(tasks)
print(f"âœ… {len(tasks)}ê°œ íƒœìŠ¤í¬ë¥¼ Label Studioë¡œ ì „ì†¡")
```

#### Step 3: ë¼ë²¨ë§ ì™„ë£Œ í›„ ìë™ ì¬í•™ìŠµ

```python```
import schedule
import time

def weekly_retrain():
    """
    ë§¤ì£¼ ì¼ìš”ì¼ ìë™ ì¬í•™ìŠµ
    """
    print("=== ì£¼ê°„ ì¬í•™ìŠµ ì‹œì‘ ===")
    
    # 1. Label Studioì—ì„œ ì™„ë£Œëœ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
    annotations = ls.get_project(project.id).get_labeled_tasks()
    
    new_data = []
    for task in annotations:
        text = task['data']['text']
        label = task['annotations']['result']['value']['choices']
        new_data.append({'clean_text': text, 'category_code': label})
    
    # 2. Supabaseì— ì €ì¥
    for item in new_data:
        supabase.table('products').insert(item).execute()
        
    # 3. ì „ì²´ ë°ì´í„° ë¡œë“œ
    all_data = supabase.table('products').select('*').execute()
    df_all = pd.DataFrame(all_data.data)
    
    # 4. ì¬í•™ìŠµ
    df_all['processed'] = df_all['clean_text'].apply(preprocessing_pipeline)
    
    X_train_new = vectorizer.fit_transform(df_all['processed'])
    model_new = LogisticRegression(max_iter=1000, C=10)
    model_new.fit(X_train_new, df_all['category_code'])
    
    # 5. ëª¨ë¸ ì €ì¥
    import joblib
    joblib.dump(model_new, 'model_v2.pkl')
    joblib.dump(vectorizer, 'vectorizer_v2.pkl')
    
    print(f"âœ… ì¬í•™ìŠµ ì™„ë£Œ! ìƒˆë¡œìš´ ë°ì´í„°: {len(new_data)}ê°œ")

# ë§¤ì£¼ ì¼ìš”ì¼ ìë™ ì‹¤í–‰
schedule.every().sunday.at("02:00").do(weekly_retrain)

while True:
    schedule.run_pending()
    time.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ì²´í¬
```

**ì˜ˆìƒ íš¨ê³¼**:[23][24][25][26]
- ì£¼ë‹¹ 100-200ê°œ ìƒˆ ë¼ë²¨ ìˆ˜ì§‘
- 4ì£¼ í›„ **+3-5% ì •í™•ë„ í–¥ìƒ**
- 6ê°œì›” í›„ **+10-15% ì •í™•ë„ í–¥ìƒ** (ì´ 93-95% ë„ë‹¬)[21][22][27]

### 3.3 Human-in-the-Loop ì›Œí¬í”Œë¡œìš°

#### React Native ì•± í”¼ë“œë°± UI

```javascript```
// React Native ì•± - ì‚¬ìš©ì í”¼ë“œë°± ìš”ì²­
import React, { useState } from 'react';
import { View, Text, Button, Picker } from 'react-native';

function FeedbackScreen({ prediction, confidence, productName }) {
  const [selectedCategory, setSelectedCategory] = useState(prediction);
  
  const handleFeedback = async () => {
    // ë°±ì—”ë“œë¡œ í”¼ë“œë°± ì „ì†¡
    await fetch('https://your-api.com/api/feedback', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        product_name: productName,
        predicted_category: prediction,
        corrected_category: selectedCategory,
        confidence: confidence
      })
    });
    
    alert('í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤! ì•±ì´ ë” ë˜‘ë˜‘í•´ì¡Œì–´ìš” ğŸ˜Š');
  };
  
  // ì‹ ë¢°ë„ 70% ë¯¸ë§Œì¼ ë•Œë§Œ í”¼ë“œë°± ìš”ì²­
  if (confidence >= 0.7) return null;
  
  return (
    <View style={{ padding: 20, backgroundColor: '#FFF3CD' }}>
      <Text style={{ fontSize: 16, marginBottom: 10 }}>
        ğŸ¤” ì´ ì œí’ˆì˜ ì¹´í…Œê³ ë¦¬ê°€ ë§ë‚˜ìš”?
      </Text>
      <Text>ì œí’ˆ: {productName}</Text>
      <Text>AI ì˜ˆì¸¡: {prediction} (ì‹ ë¢°ë„ {(confidence*100).toFixed(0)}%)</Text>
      
      <Picker
        selectedValue={selectedCategory}
        onValueChange={(value) => setSelectedCategory(value)}
      >
        <Picker.Item label="ìœ ì œí’ˆ" value="DAIRY_FRESH" />
        <Picker.Item label="ì±„ì†Œ" value="LEAFY_VEGETABLES" />
        {/* 36ê°œ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ */}
      </Picker>
      
      <Button title="í™•ì¸" onPress={handleFeedback} />
    </View>
  );
}
```

**ì‚¬ìš©ì ê²½í—˜**:
- ì‹ ë¢°ë„ 70% ì´ìƒ: ìë™ ë¶„ë¥˜ (í”¼ë“œë°± ë¶ˆí•„ìš”)
- ì‹ ë¢°ë„ 70% ë¯¸ë§Œ: ì‚¬ìš©ì í™•ì¸ ìš”ì²­ (1-2ì´ˆ ì†Œìš”)
- ë³´ìƒ: í¬ì¸íŠ¸ 10ì  ì§€ê¸‰ â†’ ì‚¬ìš©ì ì°¸ì—¬ ìœ ë„

---

## ìµœì¢… í†µí•© ì‹¤í–‰ ê³„íš

### 3ì£¼ ë¡œë“œë§µ

#### Week 1: í•©ì„± ë°ì´í„° ìƒì„± (Gemini)

**Day 1-2**: Gemini API ì„¤ì • ë° í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- Google AI Studioì—ì„œ API í‚¤ ë°œê¸‰[6]
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìµœì í™”
- 100ê°œ ìƒ˜í”Œë¡œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸

**Day 3-5**: ëŒ€ê·œëª¨ í•©ì„± ë°ì´í„° ìƒì„±
- 2,000ê°œ â†’ 20,000ê°œ ìƒì„± (10ë°° ì¦ê°•)
- í’ˆì§ˆ ê²€ì¦ (ë¬´ì‘ìœ„ 200ê°œ ìƒ˜í”Œ ì²´í¬)
- CSV ì €ì¥: `food_dataset_v5_synthetic.csv`

**ì˜ˆìƒ ê²°ê³¼**: 20,000ê°œ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°

#### Week 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ + ì¬í•™ìŠµ

**Day 1-2**: ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•
- Word2Vec í•™ìŠµ (ê¸°ì¡´ 2,000ê°œ + í•©ì„± 20,000ê°œ)
- ìë™ ë™ì˜ì–´ ì¶”ì¶œ (100ê°œ ì œí’ˆ)
- ìˆ˜ë™ ë³´ì™„ (50ê°œ í•µì‹¬ ì œí’ˆ)

**Day 3-4**: ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ + ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- ì˜ìˆ˜ì¦ íŠ¹í™” ë¶ˆìš©ì–´ 50-100ê°œ ì¶”ì¶œ
- í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- Mecab + ë™ì˜ì–´ + ë¶ˆìš©ì–´ ì ìš©

**Day 5-7**: ëª¨ë¸ ì¬í•™ìŠµ ë° í‰ê°€
- TF-IDF + LogisticRegression ì¬í•™ìŠµ
- Cross-validation (5-fold)
- í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€

**ì˜ˆìƒ ê²°ê³¼**: **75-82% ì •í™•ë„**

#### Week 3: ì•¡í‹°ë¸Œ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸

**Day 1-3**: Label Studio ì„¤ì •
- Dockerë¡œ Label Studio ì„¤ì¹˜
- Supabaseì™€ ì—°ë™
- ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œ ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•

**Day 4-5**: Uncertainty Sampling êµ¬í˜„
- Least Confidence + Margin Sampling
- ìƒìœ„ 100ê°œ ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œ ì¶”ì¶œ
- Label Studioë¡œ ì „ì†¡

**Day 6-7**: ì²« ë²ˆì§¸ ì¬í•™ìŠµ
- 100ê°œ ë¼ë²¨ë§ ì™„ë£Œ (ìˆ˜ë™)
- ìë™ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- ì£¼ê°„ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •

**ì˜ˆìƒ ê²°ê³¼**: **+3-5% ì •í™•ë„ í–¥ìƒ** â†’ **78-87%**

### ìµœì¢… ì˜ˆìƒ ì„±ê³¼

|| ë‹¨ê³„ | ë°ì´í„° ì–‘ | ì •í™•ë„ | ëˆ„ì  í–¥ìƒ |
|------|----------|--------|----------|
| **í˜„ì¬ (Baseline)** | 2,000ê°œ | 61.59% | - |
| **Week 1 (Gemini í•©ì„±)** | 20,000ê°œ | 75-78% | +13-16% |
| **Week 2 (í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§)** | 20,000ê°œ | 80-83% | +18-21% |
| **Week 3 (ì•¡í‹°ë¸Œ ëŸ¬ë‹)** | 20,100ê°œ | 83-87% | +21-25% |
| **Month 3 (ì§€ì† ê°œì„ )** | 21,000ê°œ | 85-90% | +23-28% |

---

## ë¹„ìš© ë° ROI ë¶„ì„

### ì´ ë¹„ìš©: $0

|| í•­ëª© | ë¹„ìš© | ë¹„ê³  |
|------|------|------|
| **Gemini API** | **$0** | ë¬´ë£Œ í‹°ì–´ ( í† í°/ì›”)[2][3] |
| **Label Studio****Label Studio** | **$0** | ì˜¤**Supa][23] |
| **Supabase** | **$0** | ë¬´ë£Œ í‹°ì–´ (500MB DB) |
| **ê°œë°œ ì‹œê°„** | 0ì› | ì§ì ‘ ê°œë°œ |

### ROI (íˆ¬ì ëŒ€ë¹„ íš¨ê³¼)

**íˆ¬ì**: 
- ê°œë°œ ì‹œê°„: 3ì£¼ (1ì¸ ê¸°ì¤€)
- ê¸ˆì „ ë¹„ìš©: $0

**íš¨ê³¼**:
- ì •í™•ë„ í–¥ìƒ: 61.59% â†’ 83-87% (+21-25%)
- ì‚¬ìš©ì ê²½í—˜ ê°œì„ : ì˜¤ë¶„ë¥˜ ê°ì†Œ
- ìœ ì§€ë³´ìˆ˜ ìë™í™”: ì£¼ê°„ ì¬í•™ìŠµ

**ì¥ê¸° ê°€ì¹˜**:
- 6ê°œì›” í›„ **90-95% ì •í™•ë„** ë‹¬ì„± ê°€ëŠ¥[22][27][21]
- ì‚¬ìš©ì í”¼ë“œë°± ëˆ„ì  â†’ ì§€ì†ì  ê°œì„ 
- ì™¸ë¶€ ë°ì´í„° ì˜ì¡´ë„ **0%** (ì™„ì „ ìë¦½)

***

## í•µì‹¬ ì„±ê³µ ìš”ì¸ ë° ë¦¬ìŠ¤í¬ ê´€ë¦¬

### âœ… ì„±ê³µ ìš”ì¸

1. **Gemini ë¬´ë£Œ API**: ë¬´ì œí•œ í•©ì„± ë°ì´í„° ìƒì„±]
2. **í†µì œëœ í’ˆì§ˆ**: LLM í”„ë¡¬í”„íŠ¸ë¡œ ë…¸ì´ì¦ˆ íŒ¨í„´ ì •í™•íˆ ì œì–´[13][11][12]
3. **ì•¡í‹°ë¸Œ ëŸ¬ë‹**: ë¶ˆí™•ì‹¤í•œ ë°ì´í„°ë§Œ ì„ ë³„í•˜ì—¬ íš¨ìœ¨ ê·¹ëŒ€í™”[18][20][21][22]
4. **ìë™í™”**: ì£¼ê°„ ì¬í•™ìŠµìœ¼ë¡œ ì¸ê°„ ê°œì… ìµœì†Œí™”[24][23]

### âš ï¸ ë¦¬ìŠ¤í¬ ë° ì™„í™” ë°©ì•ˆ

|| ë¦¬ìŠ¤í¬ | ì˜í–¥ | ì™„í™” ë°©ì•ˆ |
|--------|------|-----------|
| **Gemini í’ˆì§ˆ ë‚®ìŒ** | ì¤‘ê°„ | ë‹¤ë‹¨ê³„ ìƒì„± + í’ˆì§ˆ ê²€ì¦[11][12] |
| **í•œë„ ì´ˆê³¼** | ë‚®ìŒ | Gemmaë„ ì´ˆê³¼** | ë‚®ìŒ | Gemma 3 270M ë¡œì»¬ ëŒ€ì²´[7][8] |
| **Label Studio ë³µì¡** | ë‚®ìŒ | ê°„ë‹¨í•œ UI + íŠœí† ë¦¬ì–¼[28][23] |
| **ì‚¬ìš©ì í”¼ë“œë°± ë¶€ì¡±** | ì¤‘ê°„ | í¬ì¸íŠ¸ ë³´ìƒ ì‹œìŠ¤í…œ |

***

## ê²°ë¡  ë° ìµœì¢… ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì‹œì‘ (ì´ë²ˆ ì£¼)

1. **Gemini API í‚¤ ë°œê¸‰** (30ì´ˆ ì™„ë£Œ)[6][5]
2. **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±** (1ì¼)
3. **100ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸** (í’ˆì§ˆ í™•ì¸)

### í•µì‹¬ ë©”ì‹œì§€

**"í†µì œ ê°€ëŠ¥í•œ ë‚´ë¶€ ë°ì´í„°"** ì „ëµìœ¼ë¡œ **ì™¸ë¶€ ë°ì´í„° ì˜¤ì—¼ ìœ„í—˜ ì œê±°**í•˜ë©´ì„œë„ **83-87% ì •í™•ë„ ë‹¬ì„±** ê°€ëŠ¥í•©ë‹ˆë‹¤.

**3ê°€ì§€ í•µì‹¬ ì „ëµ**:
1. **Gemini í•©ì„± ë°ì´í„°** (2,000 â†’ 20,000ê°œ) â†’ +13-16%
2. **í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§** (ë™ì˜ì–´ + ë¶ˆìš©ì–´) â†’ +5-8%
3. **ì•¡í‹°ë¸Œ ëŸ¬ë‹** (ë¶ˆí™•ì‹¤í•œ ë°ì´í„° ì„ ë³„) â†’ +3-5%

**ì´ ì˜ˆìƒ ê°œì„ **: **+21-26%** (í˜„ì¬ 61.59% â†’ **83-87%**)

**ì´ ë¹„ìš©**: **$0** (Gemini ë¬´ë£Œ + ì˜¤í”ˆì†ŒìŠ¤)

**í•µì‹¬ ì°¨ë³„ì **: 
- âŒ ì™¸ë¶€ ë°ì´í„° ìŠ¤í¬ë˜í•‘ (ë²•ì  ìœ„í—˜, í’ˆì§ˆ ë¶ˆí™•ì‹¤)
- âœ… **LLM í•©ì„± ë°ì´í„°** (ì™„ì „ í†µì œ, ê³ í’ˆì§ˆ ë³´ì¥)

ì´ ì „ëµì€ **"ë°ì´í„° ì˜¤ì—¼" ì‹¤íŒ¨ ê²½í—˜**ì„ ì™„ë²½íˆ í•´ê²°í•˜ë©´ì„œë„, **85% ëª©í‘œì— ê°€ì¥ ê°€ê¹Œìš´ í˜„[11][12][23][21][1][18]

[1](https://ai.google.dev/gemini-api/docs/text-generation?hl=ko)
[2](https://ai.google.dev/gemini-api/docs/pricing)
[3](https://ai.google.dev/gemini-api/docs/pricing?hl=ko)
[4](https://www.reddit.com/r/GeminiAI/comments/1ov503t/question_massive_10_difference_in_gemini_content/)
[5](https://apidog.com/kr/blog/google-gemini-api-key-for-free-kr/)
[6](https://www.linkedin.com/pulse/step-by-step-guide-using-google-gemini-free-api-calls-image-text-y3noc)
[7](https://developers.googleblog.com/en/introducing-gemma-3-270m/)
[8](https://blog.google/technology/developers/gemma-3/)
[9](https://ai.google.dev/gemma/docs)
[10](https://www.runpod.io/articles/guides/deploying-gemma-2-for-lightweight-ai-inference-using-docker)
[11](https://aimatters.co.kr/news-report/ai-report/11725/)
[12](https://discuss.pytorch.kr/t/llm-synthetic-data-survey/4764)
[13](https://www.themoonlight.io/ko/review/synthetic-data-generation-using-large-language-models-advances-in-text-and-code)
[14](https://songai.tistory.com/69)
[15](https://wikidocs.net/50739)
[16](https://word2vec.kr)
[17](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=JAKO201833469090754)
[18](https://modal-python.readthedocs.io/en/latest/content/query_strategies/uncertainty_sampling.html)
[19](https://wikidocs.net/218105)
[20](https://spotintelligence.com/2023/08/08/active-learning-in-machine-learning/)
[21](https://intuitivetutorial.com/2021/05/15/active-learning-with-uncertainty-sampling-from-scratch/)
[22](https://lilianweng.github.io/posts/2022-02-20-active-learning/)
[23](https://www.labelvisor.com/integrating-label-studio-with-machine-learning-pipelines/)
[24](https://labelstud.io/integrations/platform/zenml/)
[25](https://labelstud.io/integrations/)
[26](https://labelstud.io/integrations/platform/)
[27](https://www.nature.com/articles/s41598-024-76800-4)
[28](https://github.com/HumanSignal/label-studio)
[29](https://ai.google.dev/gemini-api/docs/text-generation)
[30](https://cloud-allstudy.tistory.com/3460)
[31](https://docs.cloud.google.com/text-to-speech/docs/gemini-tts)
[32](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/deploy-and-inference-tutorial)
[33](https://blogs.nvidia.co.kr/blog/what-is-synthetic-data-2/)
[34](https://ai.google.dev/gemini-api/docs/models)
[35](https://www.sciencedirect.com/science/article/pii/S2666792424000271)
[36](https://arxiv.org/html/2410.17917v1)
[37](http://journal.dcs.or.kr/xml/19540/19540.pdf)
[38](https://koreascience.kr/article/CFKO201534168509089.page)
[39](https://www.zenml.io/integrations/labelstudio)
[40](https://colab.research.google.com/github/yooseonghwan/OpenDataWrangling/blob/master/03_word2vec%EC%9C%BC%EB%A1%9C_%EB%8B%A8%EC%96%B4%EC%9C%A0%EC%82%AC%EB%8F%84_%EB%B3%B4%EA%B8%B0_teacher.ipynb)